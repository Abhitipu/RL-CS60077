{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "bonq_Kq7MM1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpkFWPvfLMq2",
        "outputId": "946502de-4339-41a5-fcca-2e0cfacd36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 101 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (4.1.1)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "Hjf7KO0ZMaUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import math\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "Tx5v7VyuMd2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_bins = (6, 12)\n",
        "environment = gym.make('CartPole-v1', new_step_api = True)"
      ],
      "metadata": {
        "id": "njOBCSRJjWsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discretizer"
      ],
      "metadata": {
        "id": "E_1fXjzPMz4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discretizer( _ , __ , angle, pole_velocity):\n",
        "  \"\"\"Convert continues state intro a discrete state\"\"\"\n",
        "  lower_bounds = [ environment.observation_space.low[2], -math.radians(50) ]\n",
        "  upper_bounds = [ environment.observation_space.high[2], math.radians(50) ]\n",
        "  est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "  est.fit([lower_bounds, upper_bounds ])\n",
        "  return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
      ],
      "metadata": {
        "id": "v6HpIw62MhIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Evaluation"
      ],
      "metadata": {
        "id": "vk0tw8ntNDQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, P, R, S, discount_factor = 1.0, theta = 1e-9, max_iterations = 1e9):\n",
        "  eval_iters = 1 # Evaluation iterations\n",
        "\n",
        "  V = np.zeros(n_bins)  # Value function array\n",
        "\n",
        "  # Repeat till change in value function reaches threshold\n",
        "  for i in range(int(max_iterations)):\n",
        "    delta = 0 #Initialize change in value function to 0\n",
        "\n",
        "    # Iterate through each state\n",
        "    for angle in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        v = 0 # Accumulate expected value here\n",
        "\n",
        "        state = (angle, velocity) # state for this iteration\n",
        "\n",
        "        for action, action_probability in enumerate(policy[state]):\n",
        "          environment.state = S[state] # Initialize state of environment to pre-computed reverse map state value\n",
        "\n",
        "          #check how good next state will be\n",
        "          obs, reward, terminated, _, _ = environment.step(action)\n",
        "          next_state = discretizer(*obs)\n",
        "          # print(f'Next state={next_state}, reward = {reward}, terminated = {terminated}')\n",
        "\n",
        "          v += action_probability * P[(*state, action, *next_state)] * (R[(*state, action)] + discount_factor * V[next_state])\n",
        "\n",
        "          if terminated:\n",
        "            environment.reset()\n",
        "        \n",
        "        delta = max(delta, np.abs(V[state] - v)) # absolute change of value function\n",
        "\n",
        "        V[state] = v  # update value function\n",
        "    \n",
        "    eval_iters += 1\n",
        "\n",
        "    # Terminate if value change less than delta\n",
        "    if delta < theta:\n",
        "      print(f'Policy evaluated in {eval_iters} iterations.')\n",
        "      return V"
      ],
      "metadata": {
        "id": "_IadhmYgNGz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One step Lookahead\n",
        "For choosing the next best action from a state in a greedy member if required"
      ],
      "metadata": {
        "id": "YV0JwCK7QTNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_step_lookahead(state, V, P, R, discount_factor):\n",
        "  \"\"\"\n",
        "  Function computes the action values for different actions\n",
        "  From our state, if we take an action, how is it gonna add up to our returns\n",
        "  \"\"\"\n",
        "  action_values = np.zeros(environment.action_space.n)\n",
        "  for action in range(environment.action_space.n):\n",
        "    for angle in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        next_state = (angle, velocity)\n",
        "        action_values[action] += P[(*state, action, *next_state)] + (R[(*state, action)] + discount_factor * V[next_state])\n",
        "  return action_values"
      ],
      "metadata": {
        "id": "p5Zda0VHQdDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy iteration"
      ],
      "metadata": {
        "id": "xOB-0qwUQ74g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(P, R, S, discount_factor = 1.0, max_iterations = 1e9):\n",
        "  # start with a uniform policy\n",
        "  policy = np.ones([*n_bins, environment.action_space.n]) / environment.action_space.n\n",
        "  # Initialze counter of evaluated policies\n",
        "  eval_policies = 1\n",
        "\n",
        "  # Repeat until convergence or critical number of iterations reached\n",
        "  for i in range(int(max_iterations)):\n",
        "    stable_policy = True\n",
        "    #Evaluate current policy\n",
        "    V = policy_evaluation(policy, P, R, S, discount_factor = discount_factor)\n",
        "\n",
        "    # Go through each state and try to improve actions that were taken (policy improvement)\n",
        "    for angle in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        # Choose best action for current state\n",
        "        state = (angle, velocity)\n",
        "\n",
        "        current_action = np.argmax(policy[state])\n",
        "\n",
        "        # Look one step ahead and evaluate whether the current action is best\n",
        "        action_value = one_step_lookahead(state, V, P, R, discount_factor)\n",
        "\n",
        "        # Select better action\n",
        "        best_action = np.argmax(action_value)\n",
        "\n",
        "        # If action changes\n",
        "        if current_action != best_action:\n",
        "          stable_policy = False\n",
        "          # Greedy policy update\n",
        "          policy[state] = np.eye(environment.action_space.n)[best_action]\n",
        "\n",
        "    eval_policies += 1\n",
        "\n",
        "    # If the algorithm converged and policy is not changing anymore, then return\n",
        "    if stable_policy:\n",
        "      print(f'Evaluate {eval_policies} policies.')\n",
        "      return policy, V"
      ],
      "metadata": {
        "id": "c49Ge84IRErW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The value iteration algorithm"
      ],
      "metadata": {
        "id": "gAh1BkXiXJSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(P, R, S, discount_factor = 1.0, theta = 1e-9, max_iterations = 1e9):\n",
        "  # Initialize state-value function with zeros for each environment state\n",
        "  V = np.zeros(n_bins)\n",
        "  for i in range(int(max_iterations)):\n",
        "    # Stopping condition\n",
        "    delta = 0\n",
        "    # Update ach state\n",
        "    for angle in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        state = (angle, velocity)\n",
        "        # One ste lookahead to calculate state-action values\n",
        "        action_value = one_step_lookahead(state, V, P, R, discount_factor)\n",
        "\n",
        "        # Select best action to perform based on the highest state-action values\n",
        "        best_action_value = np.max(action_value)\n",
        "\n",
        "        # Calculate change\n",
        "        delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "\n",
        "        # Update the value function for current state\n",
        "        V[state] = best_action_value\n",
        "    \n",
        "    # Check if stopping condition:\n",
        "    if delta < theta:\n",
        "      print(f'Value-iteration converged at iterations{i}.')\n",
        "      break\n",
        "\n",
        "  # Create a deteministic policy using the optimal value function\n",
        "  policy = np.zeros([*n_bins, environment.action_space.n])\n",
        "\n",
        "  for angle in range(n_bins[0]):\n",
        "    for velocity in range(n_bins[1]):\n",
        "      state = (angle, velocity)\n",
        "      # One step lookeahead to find the best action for this state\n",
        "      action_value = one_step_lookahead(state, V, P, R, discount_factor)\n",
        "\n",
        "      # Select best action based on the highest state-action value\n",
        "      best_action = np.argmax(action_value)\n",
        "\n",
        "      # Update the policy to perform a better action at a current state\n",
        "      policy[(*state, best_action)] = 1\n",
        "  \n",
        "  return policy, V"
      ],
      "metadata": {
        "id": "8ZkA4haOXNIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_variables():  \n",
        "  P = np.zeros((*n_bins, environment.action_space.n, *n_bins))\n",
        "  N = np.zeros((*n_bins, environment.action_space.n))\n",
        "  R = np.zeros((*n_bins, environment.action_space.n))\n",
        "  S = np.zeros((*n_bins, environment.observation_space.shape[0]))\n",
        "\n",
        "  MAX_ITER = 10\n",
        "  DISCOUNT = 0.5\n",
        "  # MAX_ITER = 10000\n",
        "\n",
        "  for _ in range(MAX_ITER):\n",
        "    cur_c = environment.reset()\n",
        "    cur = discretizer(*cur_c)\n",
        "    S[cur] += cur_c\n",
        "    S[cur] *= DISCOUNT\n",
        "    \n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.randint(2)\n",
        "      # print(action)\n",
        "      # print(env.step(action))\n",
        "      obs, reward, done, _, _ = environment.step(action)\n",
        "      next_state = discretizer(*obs)\n",
        "      S[next_state] += obs\n",
        "      S[next_state] *= DISCOUNT\n",
        "      P[(*cur, action, *next_state)] += 1\n",
        "      N[(*cur, action)] += 1\n",
        "      if not done:\n",
        "        R[(*cur, action)] += 1\n",
        "      cur = next_state\n",
        "  N += 1e-10\n",
        "  R = R / N\n",
        "\n",
        "  for angle in range(n_bins[0]):\n",
        "    for velocity in range(n_bins[1]):\n",
        "      for action in range(environment.action_space.n):\n",
        "        for angle2 in range(n_bins[0]):\n",
        "          for velocity2 in range(n_bins[1]):\n",
        "            P[(angle, velocity)+ (action, ) +(angle2, velocity2)] /= N[(angle, velocity)+ (action, )]\n",
        "\n",
        "  return P, N, R, S"
      ],
      "metadata": {
        "id": "H4kzsKU5hQpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "xNwFSIiDc8co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_episodes(n_episodes, policy):\n",
        "  total_reward = 0\n",
        "  for episodes in range(n_episodes):\n",
        "    terminated = False\n",
        "    state = environment.reset()\n",
        "    state = discretizer(*state)\n",
        "    while not terminated:\n",
        "      # Select best action to perform in current state\n",
        "      action = np.argmax(policy[state])\n",
        "\n",
        "      # Perform an action and observe how environment acted in response\n",
        "      next_state, reward, terminated, info, _ = environment.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      # Update current state\n",
        "      next_state = discretizer(*next_state)\n",
        "      state = next_state\n",
        "  \n",
        "  average_reward = total_reward / n_episodes\n",
        "  return total_reward, average_reward\n",
        "\n",
        "# Number of episodes\n",
        "N_EPISODES = 10000\n",
        "# Function to find best policy\n",
        "solvers = [('Policy Iteration', policy_iteration),\n",
        "           ('Value Iteration', value_iteration)]\n",
        "\n",
        "for iteration_name, iteration_function in solvers:\n",
        "  P, N, R, S = get_variables()\n",
        "\n",
        "  environment.reset()\n",
        "  policy, V = iteration_function(P, R, S)\n",
        "\n",
        "  # Apply best policy\n",
        "  total_reward, average_reward = play_episodes(N_EPISODES, policy)\n",
        "\n",
        "  print(f'{iteration_name} :: average reward over {N_EPISODES} episodes = {average_reward} \\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKJjOXNMc_tT",
        "outputId": "ce36ea30-bd14-483f-84a7-cc764c236a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluated in 241 iterations.\n",
            "Policy evaluated in 70 iterations.\n",
            "Evaluate 3 policies.\n",
            "Policy Iteration :: average reward over 10000 episodes = 23.4268 \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in double_scalars\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value-iteration converged at iterations16.\n",
            "Value Iteration :: average reward over 10000 episodes = 9.3529 \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}