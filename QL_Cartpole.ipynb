{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "011f37c4-77f2-41a2-a1c8-a83fca3fbc0c",
      "metadata": {
        "id": "011f37c4-77f2-41a2-a1c8-a83fca3fbc0c"
      },
      "source": [
        "## Q learning with the CartPole-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06d7c09",
      "metadata": {
        "id": "a06d7c09"
      },
      "source": [
        "For a detailed description of the cartpole env, refer to [this](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) site"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr9J92PqYmFF",
        "outputId": "0878c4f7-b1ca-4f5f-dac6-5d4d8095d4d1"
      },
      "id": "qr9J92PqYmFF",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "717983ec-9919-47e4-af5b-dc3a0af603a9",
      "metadata": {
        "id": "717983ec-9919-47e4-af5b-dc3a0af603a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3833be8a-e61d-4d88-b11c-a0445c743e9a",
      "metadata": {
        "id": "3833be8a-e61d-4d88-b11c-a0445c743e9a"
      },
      "source": [
        "### Configuring the display using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "01b99802-d19f-4637-8c63-24104cc632c2",
      "metadata": {
        "id": "01b99802-d19f-4637-8c63-24104cc632c2"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "833cbce0",
      "metadata": {
        "id": "833cbce0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating some basic directories for storage"
      ],
      "metadata": {
        "id": "JVEM54bku7jw"
      },
      "id": "JVEM54bku7jw"
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDERS = ['rewards', 'graphs']\n",
        "\n",
        "for folder in FOLDERS:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)"
      ],
      "metadata": {
        "id": "X_nIHUDSuucn"
      },
      "id": "X_nIHUDSuucn",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "77431b36",
      "metadata": {
        "id": "77431b36"
      },
      "source": [
        "#### Define the policy\n",
        "<!-- $\\epsilon$ / $|A|$ + 1 - $\\epsilon$ --- $max_{a}Q(s, a)$ -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f59a582c",
      "metadata": {
        "id": "f59a582c"
      },
      "outputs": [],
      "source": [
        "def policy(Q_table, state , eps = 0.05):\n",
        "    \"\"\"\n",
        "    Epsilon greedy policy\n",
        "    Choose the next action as follows\n",
        "    eps / |A| + (1 - eps) --> greedy best\n",
        "    eps / |A|             --> random\n",
        "    \"\"\"\n",
        "    return np.argmax(Q_table[state]) if np.random.random() <= 1 - eps else np.random.randint(Q_table.shape[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26552054-8d8c-4a21-8620-b7627cf862d8",
      "metadata": {
        "id": "26552054-8d8c-4a21-8620-b7627cf862d8"
      },
      "source": [
        "### The algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "f61a5f87",
      "metadata": {
        "id": "f61a5f87"
      },
      "outputs": [],
      "source": [
        "# Helper function for rendering\n",
        "env_rgb = lambda rendered_list: np.array(rendered_list).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Q_learning(environment, discretizer, Q_table, n_episodes, render = False, discount = 1, learning_rate = 0.95):\n",
        "    \"\"\"\n",
        "    The implementation of the Q-Learning algorithm, here we take the discounts as 1 since all episodes are finite\n",
        "    \"\"\"\n",
        "    prev = 0\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    for ep in tqdm(range(n_episodes)):\n",
        "        start_state = environment.reset()\n",
        "        # print(start_state)\n",
        "        discrete_state = discretizer.discretize(*start_state)\n",
        "\n",
        "        if render:\n",
        "            img = plt.imshow(env_rgb(environment.render()))\n",
        "        \n",
        "        done = False\n",
        "        iters = 1\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            # Step 1: Choose the first action using an e-greedy policy\n",
        "            action = policy(Q_table, discrete_state, 1 / (ep + 1))\n",
        "            new_state, reward, done, info, _ = environment.step(action)\n",
        "\n",
        "            if render:\n",
        "                plt.title(f\"Episode no: {ep+1} Iteration no: {iters}\")       \n",
        "                img.set_data(env_rgb(environment.render()))\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "\n",
        "            # Step 2: Now we greedily choose the next best action and find the cumulative reward\n",
        "            new_discrete_state = discretizer.discretize(*new_state)\n",
        "            next_action = np.argmax(Q_table[new_discrete_state])\n",
        "            net_return = reward + discount * Q_table[new_discrete_state][next_action]\n",
        "            episode_reward += reward\n",
        "            if episode_reward >= 475:\n",
        "                done = True\n",
        "\n",
        "            # Step 3: Modify Q(s, a)\n",
        "            Q_table[discrete_state][action] = (1 - learning_rate) * Q_table[discrete_state][action] + learning_rate * net_return\n",
        "            \n",
        "            # Step 4: Update state\n",
        "            discrete_state = new_discrete_state\n",
        "            state = new_state\n",
        "            iters += 1\n",
        "\n",
        "        rewards[ep] = iters - 1\n",
        "        if prev < iters - 1:\n",
        "            prev = iters - 1\n",
        "#             print(f\"Episode no: {ep+1} Iterations: {iters - 1}\")\n",
        "\n",
        "    return Q_table, rewards"
      ],
      "metadata": {
        "id": "JtwwYNZlr-m-"
      },
      "id": "JtwwYNZlr-m-",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2895465a-4587-4813-9eb2-649d60f3ad92",
      "metadata": {
        "id": "2895465a-4587-4813-9eb2-649d60f3ad92"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4473464c-d733-4ed6-8552-52cb4b96031b",
      "metadata": {
        "id": "4473464c-d733-4ed6-8552-52cb4b96031b"
      },
      "outputs": [],
      "source": [
        "def play_episodes(environment, Q_res, n_episodes, discretizer, render = False):\n",
        "    \"\"\"\n",
        "    Taking steps following the Q-function, i.e. from state s, take a that maximizes Q(s, a)\n",
        "    \"\"\"\n",
        "    rewards = np.zeros(n_episodes)\n",
        "    for episode in tqdm(range(n_episodes)):\n",
        "        terminated = False\n",
        "        state = environment.reset()\n",
        "        state = discretizer.discretize(*state)\n",
        "        if render:\n",
        "            img = plt.imshow(env_rgb(environment.render()))\n",
        "\n",
        "        episode_reward = 0\n",
        "        while not terminated:\n",
        "            # Select best action to perform in a current state\n",
        "            action = np.argmax(Q_res[state])\n",
        "            # Perform an action an observe how environment acted in response\n",
        "            next_state, reward, terminated, info, _ = environment.step(action)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            \n",
        "            if episode_reward >= 475:\n",
        "                break\n",
        "            \n",
        "            if render:\n",
        "                plt.title(f\"Episode no: {episode+1} Reward: {episode_reward}\")       \n",
        "                img.set_data(env_rgb(environment.render()))\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "\n",
        "            # Update current state\n",
        "            next_state = discretizer.discretize(*next_state)\n",
        "            state = next_state\n",
        "\n",
        "        rewards[episode] = episode_reward\n",
        "\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25c9de8",
      "metadata": {
        "id": "e25c9de8"
      },
      "source": [
        "### Description\n",
        "\n",
        "#### Action space\n",
        "\n",
        "The action is an `ndarray` with shape `(1,)` which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "#### State Space\n",
        "\n",
        "The observation is an `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num |      Observation      |         Min         |        Max        |   |\n",
        "|:---:|:---------------------:|:-------------------:|:-----------------:|---|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |   |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |   |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |   |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |   |\n",
        "\n",
        "#### Rewards\n",
        "\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted. The threshold for rewards is `475` for v1.\n",
        "\n",
        "#### Starting State\n",
        "\n",
        "All observations are assigned a uniformly random value in (-0.05, 0.05)\n",
        "\n",
        "#### Episode\n",
        "\n",
        "The episode ends if any one of the following occurs:\n",
        "\n",
        "- `Termination`: Pole Angle is greater than ±12°</li>\n",
        "- `Termination`: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)</li>\n",
        "- `Truncation`: Episode length is greater than 500 (200 for v0)</li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f8b470-f8a9-4d34-92e4-9f52b5e8f232",
      "metadata": {
        "id": "d8f8b470-f8a9-4d34-92e4-9f52b5e8f232"
      },
      "source": [
        "### Discretizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discretizer:\n",
        "    \"\"\"\n",
        "    The discretizer, important for handling the continuous state space,\n",
        "    We utilize the sklearn library here\n",
        "    \"\"\"\n",
        "    def __init__(self, n_bins, lower_bounds, upper_bounds):\n",
        "        self.n_bins = n_bins\n",
        "        self.discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "        self.discretizer.fit([lower_bounds, upper_bounds])\n",
        "    \n",
        "    def discretize(self, position, velocity, angle, angular_velocity):\n",
        "        return tuple(map(int, self.discretizer.transform([[position, velocity, angle, angular_velocity]])[0]))"
      ],
      "metadata": {
        "id": "6dwbmKmlZSWW"
      },
      "id": "6dwbmKmlZSWW",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_and_plot(train_rewards, test_rewards, attr_name, attr_values, save_fig = False):\n",
        "    \"\"\"\n",
        "    This basically takes in the rewards and stores it in a pickle file after plotting a graph.\n",
        "    This is useful because it stores the data in case we need it for future analysis.\n",
        "    \"\"\"\n",
        "    attr_values = list(map(str, attr_values))\n",
        "    \n",
        "    plt.cla()\n",
        "    plt.title(f\"Rewards from training with tweaks in {attr_name}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    _ = plt.plot(np.arange(train_rewards.shape[-1]) + 1, train_rewards.T)\n",
        "    plt.legend(attr_values)\n",
        "    train_fig = plt.gca()\n",
        "    plt.show()\n",
        "    if save_fig:\n",
        "        with open(f\"rewards/train_rewards_{attr_name}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(train_fig, f)\n",
        "    plt.close()\n",
        "    \n",
        "    # Basically we print the correlations with the original value    \n",
        "    train_correlations = np.corrcoef(train_rewards)\n",
        "    print(f\"Training time correlations: {train_correlations[2,:]}\")\n",
        "    \n",
        "    mean_rewards = np.mean(test_rewards, axis=1)\n",
        "    deviations = np.std(test_rewards, axis=1)\n",
        "    print(f\"Mean: {mean_rewards}, Standard deviation: {deviations}\")\n",
        "    \n",
        "    plt.cla()\n",
        "    _ = plt.plot(np.arange(test_rewards.shape[-1]) + 1, test_rewards.T)\n",
        "    plt.title(f\"Rewards from testing with tweaks in {attr_name}\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.xlabel(\"New value\")\n",
        "    test_fig = plt.gca()\n",
        "    plt.show()\n",
        "    if save_fig:\n",
        "        with open(f\"rewards/test_rewards_{attr_name}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(test_fig, f)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "R1Dax5IJt74w"
      },
      "id": "R1Dax5IJt74w",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(env, train_episodes, test_episodes, save_data = False):\n",
        "    \"\"\"\n",
        "    The wrapper that calls the Q_learning function and returns rewards\n",
        "    \"\"\"\n",
        "    print(\"Training model\")\n",
        "    Q_table = np.zeros((*N_BINS, env.action_space.n))\n",
        "    Q_result, train_rewards = Q_learning(env, discretizer, Q_table, train_episodes, render=False)\n",
        "    \n",
        "    print(\"Testing model\")\n",
        "    test_rewards = play_episodes(env, Q_result, test_episodes, discretizer)\n",
        "    print(f'Average reward over {test_episodes} episodes = {np.mean(test_rewards)} \\n\\n')\n",
        "    \n",
        "    return train_rewards, test_rewards"
      ],
      "metadata": {
        "id": "zA1Zyxdat_ul"
      },
      "id": "zA1Zyxdat_ul",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The setup"
      ],
      "metadata": {
        "id": "iSk4xNO-cgKM"
      },
      "id": "iSk4xNO-cgKM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of partitions\n",
        "N_BINS = (6, 2, 12, 12)\n",
        "\n",
        "# Creation of environment\n",
        "environment = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
        "\n",
        "# Discretizer specifications\n",
        "low_vals = environment.observation_space.low\n",
        "high_vals = environment.observation_space.high\n",
        "\n",
        "lower_bounds = [ low_vals[0], -3.5, low_vals[2], -3.5 ]\n",
        "upper_bounds = [ high_vals[0], 3.5, high_vals[2], 3.5 ]\n",
        "\n",
        "discretizer = Discretizer(N_BINS, lower_bounds, upper_bounds)"
      ],
      "metadata": {
        "id": "QSjd23jYbwoB"
      },
      "id": "QSjd23jYbwoB",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The runner"
      ],
      "metadata": {
        "id": "XZ8GudVHzg2S"
      },
      "id": "XZ8GudVHzg2S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc4e899",
      "metadata": {
        "id": "0dc4e899"
      },
      "outputs": [],
      "source": [
        "N_TRAIN_EPISODES = 50000\n",
        "N_TEST_EPISODES = 1000\n",
        "\n",
        "PLOT = True\n",
        "SAVE = True\n",
        "\n",
        "attributes = [\"gravity\", \"masscart\", \"masspole\", \"length\", \"force_mag\"]\n",
        "\n",
        "for attr in attributes:\n",
        "    print(f\"Tweaking {attr}\")\n",
        "    print(f\"Tweaking {attr}\", file=sys.stderr)\n",
        "    \n",
        "    # obtain original value\n",
        "    orig_value = getattr(environment, attr)\n",
        "    \n",
        "    all_train_rewards = np.zeros((5, N_TRAIN_EPISODES))\n",
        "    all_test_rewards = np.zeros((5, N_TEST_EPISODES))\n",
        "    \n",
        "    new_values = np.linspace(0, 2 * orig_value, 5)\n",
        "    new_values = np.round(new_values, decimals = 2)\n",
        "    \n",
        "    for idx, value in enumerate(new_values):\n",
        "        setattr(environment, attr, value)\n",
        "        train_rewards, test_rewards = run_model(environment, N_TRAIN_EPISODES, N_TEST_EPISODES, save_data = True) \n",
        "        print(f\"Mean {np.mean(test_rewards)} and deviation {np.std(test_rewards)}\")\n",
        "        all_train_rewards[idx] = train_rewards\n",
        "        all_test_rewards[idx] = test_rewards\n",
        "    \n",
        "    if PLOT:\n",
        "        analyze_and_plot(all_train_rewards, all_test_rewards, attr, new_values, save_fig = SAVE)\n",
        "        \n",
        "    # reset to original value\n",
        "    setattr(environment, attr, orig_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis\n",
        "Here we retrieve data from the pickle files and carry out our analysis"
      ],
      "metadata": {
        "id": "9i5AAxvSuYPB"
      },
      "id": "9i5AAxvSuYPB"
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = [\"gravity\", \"masscart\", \"masspole\", \"length\", \"force_mag\"]\n",
        "WINDOW_SIZE = 2000\n",
        "\n",
        "X = np.arange(0, N_TRAIN_EPISODES, WINDOW_SIZE)\n",
        "\n",
        "def save_train(attr, train_rewards, new_values, X):\n",
        "    plt.cla()\n",
        "    print(\"Train\")\n",
        "    plt.title(f\"Rewards from training with tweaks in {attr}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    _ = plt.plot(X, np.array(train_rewards).T)\n",
        "    _ = plt.legend(new_values)\n",
        "    plt.savefig(f\"graphs/train_rewards_{attr}.jpg\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def save_test(attr, new_values, mu):\n",
        "    plt.cla()\n",
        "    print(\"Test\")\n",
        "    plt.title(f\"Mean rewards from testing with tweaks in {attr}\")\n",
        "    plt.xlabel(f\"New values of {attr}\")\n",
        "    plt.ylabel(\"Mean reward\")\n",
        "    _ = plt.bar(new_values, mu)\n",
        "    plt.savefig(f\"graphs/test_rewards_{attr}.jpg\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "for attr in attributes:\n",
        "    with open(f\"rewards/train_rewards_{attr}.pkl\", \"rb\") as f:\n",
        "        axes = pickle.load(f)\n",
        "\n",
        "    orig_value = getattr(environment, attr)\n",
        "    new_values = np.linspace(0, 2 * orig_value, 5)\n",
        "    new_values = np.round(new_values, decimals = 2)\n",
        "    new_values = list(map(str, new_values))\n",
        "    \n",
        "    train_rewards = []\n",
        "    \n",
        "    for line in axes.lines:\n",
        "        my_rewards = np.array(line.get_ydata())\n",
        "        train_rewards.append(np.convolve(my_rewards, np.ones(WINDOW_SIZE))[X] / WINDOW_SIZE)\n",
        "        \n",
        "    save_train(attr, train_rewards, new_values, X)   \n",
        "\n",
        "    with open(f\"rewards/test_rewards_{attr}.pkl\", \"rb\") as f:\n",
        "        axes = pickle.load(f)\n",
        "    \n",
        "    test_rewards = []\n",
        "    for line in axes.lines:\n",
        "        my_rewards = np.array(line.get_ydata())\n",
        "        test_rewards.append(my_rewards)\n",
        "    test_rewards = np.array(test_rewards)\n",
        "    mu = np.mean(test_rewards, axis=1)\n",
        "\n",
        "    save_test(attr, new_values, mu)"
      ],
      "metadata": {
        "id": "qOSgD9DEuabQ"
      },
      "id": "qOSgD9DEuabQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}