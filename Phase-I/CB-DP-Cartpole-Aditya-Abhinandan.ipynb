{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change in Behavior and Transfer with Change in Properties of the Environment\n",
        "\n",
        "Team Members\n",
        "- Aditya Anantwar 19CS10006\n",
        "- Abhinandan De 19CS10069\n",
        "\n",
        "### DP implementation on CartPole-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aeP2KbOkxdP"
      },
      "outputs": [],
      "source": [
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dslnLQg4k00p"
      },
      "outputs": [],
      "source": [
        "%reset -f array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMt6CeiNk3HW"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import math\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVdBRv2qhqpU"
      },
      "source": [
        "### Description\n",
        "\n",
        "#### Action space\n",
        "\n",
        "The action is an `ndarray` with shape `(1,)` which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "#### State Space\n",
        "\n",
        "The observation is an `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num |      Observation      |         Min         |        Max        |   |\n",
        "|:---:|:---------------------:|:-------------------:|:-----------------:|---|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |   |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |   |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |   |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |   |\n",
        "\n",
        "#### Rewards\n",
        "\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted. The threshold for rewards is `475` for v1.\n",
        "\n",
        "#### Starting State\n",
        "\n",
        "All observations are assigned a uniformly random value in (-0.05, 0.05)\n",
        "\n",
        "#### Episode\n",
        "\n",
        "The episode ends if any one of the following occurs:\n",
        "\n",
        "- `Termination`: Pole Angle is greater than ±12°</li>\n",
        "- `Termination`: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)</li>\n",
        "- `Truncation`: Episode length is greater than 500 (200 for v0)</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLQE8dXuk5Qt"
      },
      "outputs": [],
      "source": [
        "n_bins = (6, 2, 12, 12)\n",
        "env = gym.make('CartPole-v1', new_step_api = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KKtqwV6k-kK"
      },
      "outputs": [],
      "source": [
        "lower_bounds = [ env.observation_space.low[0], -3.5, env.observation_space.low[2], -3.5 ]\n",
        "upper_bounds = [ env.observation_space.high[0], 3.5, env.observation_space.high[2], 3.5 ]\n",
        "est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "est.fit([lower_bounds, upper_bounds ])\n",
        "\n",
        "def discretizer( position , velocity , angle, pole_velocity):\n",
        "  \"\"\"Convert continues state intro a discrete state\"\"\"  \n",
        "  return tuple(map(int,est.transform([[position, velocity, angle, pole_velocity]])[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjZEHdpUlAdp"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(environment, policy, P, R, discount_factor = 1.0, theta = 1e-1, max_iterations = 1e9):\n",
        "  eval_iters = 1 # Evaluation iterations\n",
        "\n",
        "  V = np.zeros(n_bins)  # Value function array\n",
        "\n",
        "  # Repeat till change in value function reaches threshold\n",
        "  for i in range(int(max_iterations)):\n",
        "    delta = 0 #Initialize change in value function to 0\n",
        "\n",
        "    # Iterate through each state\n",
        "    for position in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        for angle in range(n_bins[2]):\n",
        "          for pole_velocity in range(n_bins[3]):\n",
        "            v = 0 # Accumulate expected value here\n",
        "\n",
        "            state = (position, velocity, angle, pole_velocity) # state for this iteration\n",
        "\n",
        "            for action, action_probability in enumerate(policy[state]):\n",
        "              # environment.state = S[state] # Initialize state of environment to pre-computed reverse map state value\n",
        "\n",
        "              #check how good next state will be\n",
        "              for position2 in range(n_bins[0]):\n",
        "                for velocity2 in range(n_bins[1]):\n",
        "                  for angle2 in range(n_bins[2]):\n",
        "                    for pole_velocity2 in range(n_bins[3]): \n",
        "                      next_state = (position2, velocity2, angle2, pole_velocity2)\n",
        "                      v += action_probability * P[(*state, action, *next_state)] * (R[(*state, action)] + discount_factor * V[next_state])\n",
        "            \n",
        "            delta = max(delta, np.abs(V[state] - v)) # absolute change of value function\n",
        "\n",
        "            V[state] = v  # update value function\n",
        "    \n",
        "    eval_iters += 1\n",
        "    print(f\"iteration: {eval_iters}, delta = {delta}\")\n",
        "\n",
        "    # Terminate if value change less than delta\n",
        "    if delta < theta:\n",
        "      print(f'Policy evaluated in {eval_iters} iterations.')\n",
        "      return V\n",
        "  return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x53Ls83glCrg"
      },
      "outputs": [],
      "source": [
        "def one_step_lookahead(environment, state, V, P, R, discount_factor):\n",
        "  \"\"\"\n",
        "  Function computes the action values for different actions\n",
        "  From our state, if we take an action, how is it gonna add up to our returns\n",
        "  \"\"\"\n",
        "  action_values = np.zeros(environment.action_space.n)\n",
        "  for action in range(environment.action_space.n):\n",
        "    for position in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        for angle in range(n_bins[2]):\n",
        "          for pole_velocity in range(n_bins[3]):\n",
        "            next_state = (position, velocity, angle, pole_velocity)\n",
        "            action_values[action] += P[(*state, action, *next_state)] * (R[(*state, action)] + discount_factor * V[next_state])\n",
        "  return action_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FkIKLNlEli"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(environment, P, R, discount_factor = 1.0, max_iterations = 1e9):\n",
        "  # start with a uniform policy\n",
        "  policy = np.ones([*n_bins, environment.action_space.n]) / environment.action_space.n\n",
        "  # Initialze counter of evaluated policies\n",
        "  eval_policies = 1\n",
        "\n",
        "  # Repeat until convergence or critical number of iterations reached\n",
        "  for i in range(int(max_iterations)):\n",
        "    stable_policy = True\n",
        "    #Evaluate current policy\n",
        "    V = policy_evaluation(environment, policy, P, R, discount_factor = discount_factor, max_iterations = max_iterations)\n",
        "\n",
        "    # Go through each state and try to improve actions that were taken (policy improvement)\n",
        "    for position in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        for angle in range(n_bins[2]):\n",
        "          for pole_velocity in range(n_bins[3]):\n",
        "            # Choose best action for current state\n",
        "            state = (position, velocity, angle, pole_velocity)\n",
        "\n",
        "            current_action = np.argmax(policy[state])\n",
        "\n",
        "            # Look one step ahead and evaluate whether the current action is best\n",
        "            action_value = one_step_lookahead(environment, state, V, P, R, discount_factor)\n",
        "\n",
        "            # Select better action\n",
        "            best_action = np.argmax(action_value)\n",
        "\n",
        "            # If action changes\n",
        "            if current_action != best_action:\n",
        "              stable_policy = False\n",
        "              # Greedy policy update\n",
        "              policy[state] = np.eye(environment.action_space.n)[best_action]\n",
        "\n",
        "    eval_policies += 1\n",
        "\n",
        "    # If the algorithm converged and policy is not changing anymore, then return\n",
        "    if stable_policy:\n",
        "      print(f'Evaluate {eval_policies} policies.')\n",
        "      return policy, V\n",
        "  return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAowllJ1lGWN"
      },
      "outputs": [],
      "source": [
        "def value_iteration(environment, P, R, discount_factor = 1.0, theta = 1e-1, max_iterations = 1e9):\n",
        "  # Initialize state-value function with zeros for each environment state\n",
        "  V = np.zeros(n_bins)\n",
        "  for i in range(int(max_iterations)):\n",
        "    # Stopping condition\n",
        "    delta = 0\n",
        "    # Update ach state\n",
        "    for position in range(n_bins[0]):\n",
        "      for velocity in range(n_bins[1]):\n",
        "        for angle in range(n_bins[2]):\n",
        "          for pole_velocity in range(n_bins[3]):\n",
        "            state = (position, velocity, angle, pole_velocity)\n",
        "            # One ste lookahead to calculate state-action values\n",
        "            action_value = one_step_lookahead(environment, state, V, P, R, discount_factor)\n",
        "\n",
        "            # Select best action to perform based on the highest state-action values\n",
        "            best_action_value = np.max(action_value)\n",
        "\n",
        "            # Calculate change\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "\n",
        "            # Update the value function for current state\n",
        "            V[state] = best_action_value\n",
        "    \n",
        "    # Check if stopping condition:\n",
        "    if delta < theta:\n",
        "      print(f'Value-iteration converged at iterations {i}.')\n",
        "      break\n",
        "\n",
        "  # Create a deteministic policy using the optimal value function\n",
        "  policy = np.zeros([*n_bins, environment.action_space.n])\n",
        "\n",
        "  for position in range(n_bins[0]):\n",
        "    for velocity in range(n_bins[1]):\n",
        "      for angle in range(n_bins[2]):\n",
        "        for pole_velocity in range(n_bins[3]):\n",
        "          state = (position, velocity, angle, pole_velocity)\n",
        "          # One step lookeahead to find the best action for this state\n",
        "          action_value = one_step_lookahead(environment, state, V, P, R, discount_factor)\n",
        "\n",
        "          # Select best action based on the highest state-action value\n",
        "          best_action = np.argmax(action_value)\n",
        "\n",
        "          # Update the policy to perform a better action at a current state\n",
        "          policy[(*state, best_action)] = 1\n",
        "  \n",
        "  return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHI9uaxQlIBq"
      },
      "outputs": [],
      "source": [
        "def get_variables(env):  \n",
        "  P = np.zeros((*n_bins, env.action_space.n, *n_bins))\n",
        "  N = np.zeros((*n_bins, env.action_space.n))\n",
        "  R = np.zeros((*n_bins, env.action_space.n))\n",
        "\n",
        "  MAX_ITER = 1000\n",
        "  DISCOUNT = 0.5\n",
        "\n",
        "  for _ in range(MAX_ITER):\n",
        "    cur_c = env.reset()\n",
        "    cur = discretizer(*cur_c)\n",
        "    \n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.randint(2)\n",
        "      obs, reward, done, _, _ = env.step(action)\n",
        "      next_state = discretizer(*obs)\n",
        "      P[(*cur, action, *next_state)] += 1\n",
        "      N[(*cur, action)] += 1\n",
        "      if not done:\n",
        "        R[(*cur, action)] += 1\n",
        "      cur = next_state\n",
        "  N += 1e-9\n",
        "  R = R / N\n",
        "\n",
        "  P = P / N.reshape(*n_bins, env.action_space.n, 1, 1, 1, 1)\n",
        "\n",
        "  return P, N, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wjOOcZhlJjd"
      },
      "outputs": [],
      "source": [
        "def play_episodes(environment, n_episodes, policy):\n",
        "  total_reward = 0\n",
        "  for episodes in range(n_episodes):\n",
        "    terminated = False\n",
        "    state = environment.reset()\n",
        "    state = discretizer(*state)\n",
        "    while not terminated:\n",
        "      # Select best action to perform in current state\n",
        "      action = np.argmax(policy[state])\n",
        "\n",
        "      # Perform an action and observe how environment acted in response\n",
        "      next_state, reward, terminated, info, _ = environment.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      # Update current state\n",
        "      next_state = discretizer(*next_state)\n",
        "      state = next_state\n",
        "  \n",
        "  average_reward = total_reward / n_episodes\n",
        "  return total_reward, average_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3r1NzcFlLhl"
      },
      "outputs": [],
      "source": [
        "def run_model(environment, max_iterations, test_episodes):\n",
        "  # Number of episodes\n",
        "  N_EPISODES = 10000\n",
        "  # Function to find best policy\n",
        "  solvers = [('Policy Iteration', policy_iteration),\n",
        "            ('Value Iteration', value_iteration)]\n",
        "\n",
        "  P, N, R = get_variables(environment)\n",
        "\n",
        "  rewards = {'Policy Iteration': 0,\n",
        "            'Value Iteration': 0}\n",
        "\n",
        "  for iteration_name, iteration_function in solvers:\n",
        "    environment.reset()\n",
        "    policy, V = iteration_function(environment, P, R, max_iterations = max_iterations)\n",
        "\n",
        "    # Apply best policy\n",
        "    total_reward, average_reward = play_episodes(environment, N_EPISODES, policy)\n",
        "\n",
        "    rewards[iteration_name] = average_reward\n",
        "\n",
        "    print(f'{iteration_name} :: average reward over {N_EPISODES} episodes = {average_reward} \\n\\n')\n",
        "  \n",
        "  return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Oma0xKjkSg"
      },
      "outputs": [],
      "source": [
        "MAX_ITERS = 10\n",
        "N_TEST_EPISODES = 1000\n",
        "PLOT = True\n",
        "\n",
        "attributes = [\"gravity\", \"masscart\", \"masspole\", \"length\", \"force_mag\"]\n",
        "\n",
        "environment = gym.make('CartPole-v1', new_step_api = True)\n",
        "\n",
        "for attr in attributes:\n",
        "    print(f\"Tweaking {attr}\")\n",
        "    \n",
        "    # obtain original value\n",
        "    orig_value = getattr(environment, attr)\n",
        "    \n",
        "    all_test_rewards = []\n",
        "    \n",
        "    new_values = np.linspace(0, 2 * orig_value, 5)\n",
        "    new_values = np.round(new_values, decimals = 2)\n",
        "    \n",
        "    for idx, value in enumerate(new_values):\n",
        "        setattr(environment, attr, value)\n",
        "        test_rewards = run_model(environment, MAX_ITERS, N_TEST_EPISODES)\n",
        "        \n",
        "        all_test_rewards.append(test_rewards)\n",
        "    \n",
        "    with open('rewards.txt', 'a') as f:\n",
        "      for item in all_test_rewards:\n",
        "        for key, value in item.items():\n",
        "          f.write(f'{attr} {key} {value}\\n')\n",
        "        \n",
        "    # reset to original value\n",
        "    setattr(environment, attr, orig_value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a4970bc424d9306b5a36e6185cf477d7f33862c0774278df221e7f032971662"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
